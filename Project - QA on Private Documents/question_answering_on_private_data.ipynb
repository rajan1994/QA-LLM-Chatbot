{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project: Question- Answering on Private Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# loading env variables from .env file\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(file):\n",
    "    \"\"\"\n",
    "    Load documents from a file.\n",
    "    :param file: File to load\n",
    "    :return: List of documents\n",
    "    \"\"\"\n",
    "    extension = file.split(\".\")[-1]\n",
    "    if extension == \"txt\":\n",
    "        from langchain.document_loaders import TextLoader\n",
    "        print(f\"Loading text document {file}\")\n",
    "        loader = TextLoader(file)\n",
    "    elif extension == \"json\":\n",
    "        from langchain.document_loaders import JSONLoader\n",
    "        print(f\"Loading json document {file}\")\n",
    "        loader = JSONLoader(file)\n",
    "    elif extension == \"csv\":\n",
    "        from langchain.document_loaders import CSVLoader\n",
    "        print(f\"Loading csv document {file}\")\n",
    "        loader = CSVLoader(file)\n",
    "    elif extension in [\"doc\",\"docx\"]:\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f\"Loading docx document {file}\")\n",
    "        loader = Docx2txtLoader(file)\n",
    "    elif extension == \"pdf\":\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f\"Loading pdf document {file}\")\n",
    "        loader = PyPDFLoader(file)\n",
    "    else:\n",
    "        print(\"Document type not supported\")\n",
    "        return None\n",
    "    data = loader.load()\n",
    "    return data\n",
    "\n",
    "def load_wikipedia(query, lang=\"en\",load_max_docs=2):\n",
    "    \"\"\"\n",
    "    Load wikipedia articles from the wikipedia API\n",
    "    \"\"\"\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query, lang, load_max_docs)\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=300):\n",
    "    \"\"\"\n",
    "    Split data into chunks based on \\n or .\n",
    "    \"\"\"\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=20)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_cost(texts, model = 'text-embedding-ada-002', token_cost = 0.0004):\n",
    "    \"\"\"\n",
    "    Calculates the cost of encoding the texts using the specified model.\n",
    "    \"\"\"\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    total_cost = f'Total cost: {total_tokens /1000 * token_cost:.6f}$'\n",
    "    return total_tokens, total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding and Uploading to a Vector Database (Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embedding(index_name, chunks, model = \"text-embedding-ada-002\"):\n",
    "    \"\"\"\n",
    "    Inserts or fetches a vector store from Pinecone.\n",
    "    \"\"\"\n",
    "    import pinecone\n",
    "    import time\n",
    "    from langchain.vectorstores import Pinecone\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model=model)\n",
    "    \n",
    "    pinecone.init(api_key=os.environ.get(\"PINECONE_API_KEY\"), environment=os.environ.get(\"PINECONE_API_ENV\"))\n",
    "\n",
    "    if index_name in pinecone.list_indexes():\n",
    "        print(f\"Index {index_name} already exists, fetching...\")\n",
    "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "    else:\n",
    "        print(f\"Index {index_name} does not exist, creating...\")\n",
    "        pinecone.create_index(index_name, dimension=1536, metric=\"cosine\")\n",
    "        while index_name not in pinecone.list_indexes():\n",
    "            time.sleep(10)\n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name = \"all\"):\n",
    "    \"\"\"\n",
    "    Delete a pinecone index.\n",
    "    \"\"\"\n",
    "    import pinecone\n",
    "    \n",
    "    pinecone.init(api_key=os.environ.get(\"PINECONE_API_KEY\"), environment=os.environ.get(\"PINECONE_API_ENV\"))\n",
    "\n",
    "    if index_name==\"all\":\n",
    "        indexes = pinecone.list_indexes()\n",
    "        for index in indexes:\n",
    "            pinecone.delete_index(index)\n",
    "        print(\"Deleted pinecone indexes: \" + str(indexes))\n",
    "    elif index_name is not None:\n",
    "        pinecone.delete_index(index_name)\n",
    "        print(\"Deleted pinecone index: \" + index_name)\n",
    "    else:\n",
    "        print(\"There are no index present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asking Questions and Getting Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_ques_and_get_answer(vector_store, ques):\n",
    "    \"\"\"\n",
    "    This function takes in a vector store and a question and returns the answer.\n",
    "    \"\"\"\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_params={'k': 5})\n",
    "\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "    answer = chain.run(ques)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def ask_with_memory(vector_store, ques, chat_history=[]):\n",
    "    \"\"\"\n",
    "    This function takes in a vector store, a question, and a chat history and returns the answer with the chat history.\n",
    "    \"\"\"\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_params={'k': 3})\n",
    "\n",
    "    chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)\n",
    "    result = chain({'question': ques, 'chat_history': chat_history})\n",
    "    chat_history.append((ques, result['answer']))\n",
    "    \n",
    "    return result, chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running code from document files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 files in the folder\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['files/attention_is_all_you_need.pdf',\n",
       " 'files/react.pdf',\n",
       " 'files/CDOC-110hdoc50.pdf',\n",
       " 'files/state_of_the_union.txt',\n",
       " 'files/sj.txt',\n",
       " 'files/churchill_speech.txt',\n",
       " 'files/us_constitution.pdf',\n",
       " 'files/the_great_gatsby.docx']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_path = []\n",
    "directory_path = \"files\"\n",
    "# Reading all the files from files folder\n",
    "for file in os.listdir(directory_path):\n",
    "    files_path.append(os.path.join(directory_path, file))\n",
    "\n",
    "print(f\"There are {len(files_path)} files in the folder\")\n",
    "files_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pdf document files/attention_is_all_you_need.pdf\n",
      "Loading pdf document files/react.pdf\n",
      "Loading pdf document files/CDOC-110hdoc50.pdf\n",
      "Loading text document files/state_of_the_union.txt\n",
      "Loading text document files/sj.txt\n",
      "Loading text document files/churchill_speech.txt\n",
      "Loading pdf document files/us_constitution.pdf\n",
      "Loading docx document files/the_great_gatsby.docx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "stats = []\n",
    "doc_chunks = []\n",
    "for doc in files_path:\n",
    "    data = load_documents(doc)\n",
    "    chunks = chunk_data(data)\n",
    "    doc_chunks.extend(chunks)\n",
    "    total_tokens, total_cost = calculating_cost(chunks)\n",
    "    stats.append([doc, len(chunks), total_tokens, total_cost])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>chunks</th>\n",
       "      <th>tokens</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>files/attention_is_all_you_need.pdf</td>\n",
       "      <td>156</td>\n",
       "      <td>10232</td>\n",
       "      <td>Total cost: 0.004093$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>files/react.pdf</td>\n",
       "      <td>428</td>\n",
       "      <td>33676</td>\n",
       "      <td>Total cost: 0.013470$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>files/CDOC-110hdoc50.pdf</td>\n",
       "      <td>1336</td>\n",
       "      <td>68539</td>\n",
       "      <td>Total cost: 0.027416$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>files/state_of_the_union.txt</td>\n",
       "      <td>166</td>\n",
       "      <td>8089</td>\n",
       "      <td>Total cost: 0.003236$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>files/sj.txt</td>\n",
       "      <td>53</td>\n",
       "      <td>2766</td>\n",
       "      <td>Total cost: 0.001106$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>files/churchill_speech.txt</td>\n",
       "      <td>84</td>\n",
       "      <td>4621</td>\n",
       "      <td>Total cost: 0.001848$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>files/us_constitution.pdf</td>\n",
       "      <td>177</td>\n",
       "      <td>17660</td>\n",
       "      <td>Total cost: 0.007064$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>files/the_great_gatsby.docx</td>\n",
       "      <td>1104</td>\n",
       "      <td>69326</td>\n",
       "      <td>Total cost: 0.027730$</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  file  chunks  tokens                   cost\n",
       "0  files/attention_is_all_you_need.pdf     156   10232  Total cost: 0.004093$\n",
       "1                      files/react.pdf     428   33676  Total cost: 0.013470$\n",
       "2             files/CDOC-110hdoc50.pdf    1336   68539  Total cost: 0.027416$\n",
       "3         files/state_of_the_union.txt     166    8089  Total cost: 0.003236$\n",
       "4                         files/sj.txt      53    2766  Total cost: 0.001106$\n",
       "5           files/churchill_speech.txt      84    4621  Total cost: 0.001848$\n",
       "6            files/us_constitution.pdf     177   17660  Total cost: 0.007064$\n",
       "7          files/the_great_gatsby.docx    1104   69326  Total cost: 0.027730$"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_stats = pd.DataFrame(stats, columns=['file', 'chunks', 'tokens', 'cost'])\n",
    "file_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted pinecone indexes: ['document-index']\n"
     ]
    }
   ],
   "source": [
    "# deleing all pinecone existing index\n",
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index document-index does not exist, creating...\n"
     ]
    }
   ],
   "source": [
    "# creating an index and inserting documents into it\n",
    "index_name = \"document-index\"\n",
    "vector_store = insert_or_fetch_embedding(index_name, doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document is the Constitution of the United States of America.\n"
     ]
    }
   ],
   "source": [
    "# Single question query\n",
    "ques = 'What is the whole document about?'\n",
    "answer = ask_ques_and_get_answer(vector_store, ques)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Quit or Exit to quit.\n",
      "Question: What does US constitution states?\n",
      "Answer: The United States Constitution states the purpose of forming a more perfect union, establishing justice, ensuring domestic tranquility, providing for the common defense, promoting the general welfare, and securing the blessings of liberty for ourselves and future generations. It also establishes the Constitution as the supreme law of the land, along with laws made in accordance with it and treaties of the United States. The Constitution binds judges in every state.\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "Question: What are the different documents data you have?\n",
      "Answer: Based on the provided context, it is not clear what specific documents or data are being referred to. The context mentions \"acts, records, and proceedings,\" but it does not provide any specific information about the content or nature of these documents. Therefore, it is not possible to determine the different documents or data that are available.\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "Quitting ... bye bye!\n"
     ]
    }
   ],
   "source": [
    "# Ask any no of questions till we quit\n",
    "import time\n",
    "i = 1\n",
    "print('Write Quit or Exit to quit.')\n",
    "while True:\n",
    "    q = input(f'Question #{i}: ')\n",
    "    i = i + 1\n",
    "    if q.lower() in ['quit', 'exit']:\n",
    "        print('Quitting ... bye bye!')\n",
    "        time.sleep(2)\n",
    "        break\n",
    "    \n",
    "    answer = ask_ques_and_get_answer(vector_store, q)\n",
    "    print(f'Question: {q}\\nAnswer: {answer}')\n",
    "    print(f'\\n {\"-\" * 50} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Downloading public content from Wikipedia and perform Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted pinecone indexes: ['document-index']\n"
     ]
    }
   ],
   "source": [
    "# deleting all indexes\n",
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index chatgpt does not exist, creating...\n"
     ]
    }
   ],
   "source": [
    "data = load_wikipedia('ChatGPT', 'en')\n",
    "chunks = chunk_data(data)\n",
    "index_name = 'chatgpt'\n",
    "vector_store = insert_or_fetch_embedding(index_name, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ChatGPT, short for Chat Generative Pre-trained Transformer, is a chatbot developed by OpenAI. It is a language model-based chatbot that allows users to have conversations with it. It is built on the transformer architecture and has been fine-tuned specifically for conversational applications. ChatGPT was released as a research preview and is available for free, but there is also a paid version called \"ChatGPT Plus\" that offers additional features and priority access to newer updates.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is ChatGPT?\"\n",
    "answer = ask_ques_and_get_answer(vector_store, query)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How many model ChatGPT is having? List down the names.',\n",
       " 'chat_history': [('How many model ChatGPT is having? List down the names.',\n",
       "   'ChatGPT is based on two GPT foundation models: GPT-3.5 and GPT-4. These models were fine-tuned specifically for conversational usage to create the chatbot product.')],\n",
       " 'answer': 'ChatGPT is based on two GPT foundation models: GPT-3.5 and GPT-4. These models were fine-tuned specifically for conversational usage to create the chatbot product.'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"How many model ChatGPT is having? List down the names.\"\n",
    "answer, chat_history = ask_with_memory(vector_store, query, chat_history)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('How many model ChatGPT is having? List down the names.',\n",
       "  'ChatGPT is based on two GPT foundation models: GPT-3.5 and GPT-4. These models were fine-tuned specifically for conversational usage to create the chatbot product.')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"How many model ChatGPT is having? List down the names.\"\n",
    "answer, chat_history = ask_with_memory(vector_store, query, chat_history)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model names mentioned in the previous answer are GPT-1, GPT-2, GPT-3, GPT-3.5, and GPT-4.\n",
      "[('How many model ChatGPT is having? List down the names.', 'ChatGPT is based on two GPT foundation models: GPT-3.5 and GPT-4. These models were fine-tuned specifically for conversational usage to create the chatbot product.'), ('Multiply the model count by 2', 'The information provided does not mention the \"model count\" specifically. Therefore, I do not have the necessary context to determine the result of multiplying the model count by 2.'), ('Multiply the number count by 2', \"I'm sorry, but I don't have enough information to answer your question.\"), ('Give me the model names from the above answer', 'The model names mentioned in the previous answer are GPT-1, GPT-2, GPT-3, GPT-3.5, and GPT-4.')]\n"
     ]
    }
   ],
   "source": [
    "question = 'Give me the model names from the above answer'\n",
    "result, chat_history = ask_with_memory(vector_store, question, chat_history)\n",
    "print(result['answer'])\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ask with Memory Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "i = 1\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "print(\"Write Quit or Exit to quit\")\n",
    "while True:\n",
    "    q = input(f\"Question #{i}\")\n",
    "    i = i + 1\n",
    "    if q.lower() in [\"quit\",\"exit\"]:\n",
    "        print(\"Quitting\")\n",
    "        time.sleep(2)\n",
    "        break\n",
    "    result, _ = ask_with_memory(vector_store, q, chat_history)\n",
    "    print (result['answer'])\n",
    "    print(\"----------------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
